<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Webcam con MediaPipe Face Detector</title>
    
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/vision_bundle.js"
        crossorigin="anonymous"></script>

    <style>
        /* (CSS sin cambios importantes, manteniendo el contenedor y el volteo) */
        body {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            min-height: 100vh;
            margin: 0;
            background-color: #f0f0f0;
            font-family: sans-serif;
        }

        .camera-container {
            position: relative;
            width: 100%;
            max-width: 800px;
            overflow: hidden;
            border-radius: 10px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
            aspect-ratio: 4 / 3; 
        }

        #camera-preview {
            width: 100%;
            height: 100%;
            object-fit: cover;
            display: block;
            transform: scaleX(-1); /* Volteo de espejo */
        }

        /* El SVG del overlay est√°tico (gu√≠a del rostro) */
        #overlay-svg {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            pointer-events: none; 
        }

        /* El Canvas para dibujar la detecci√≥n */
        #detection-canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            pointer-events: none;
            /* IMPORTANTE: NO VOLTEAMOS EL CANVAS con MediaPipe
               MediaPipe nos da coordenadas absolutas del video original.
               ¬°El volteo se hace manualmente en el c√≥digo al dibujar!
            */
        }
    </style>
</head>
<body>

    <h1>MediaPipe Detecci√≥n de Rostro</h1>
    <p id="loading-message" style="text-align: center;">Cargando modelos de MediaPipe... üß†</p>
    
    <div class="camera-container">
        
        <video id="camera-preview" autoplay playsinline muted></video>

        <svg id="overlay-svg" viewBox="0 0 100 100" preserveAspectRatio="none">
            <path fill-rule="evenodd" 
                  fill="rgba(255, 255, 255, 0.85)"
                  d="M 0 0 H 100 V 100 H 0 Z 
                     M 50 30 
                     A 15 20 0 1 1 50 70 
                     A 15 20 0 1 1 50 30 Z">
            </path>
            <path fill="none"
                  stroke="#007bff"
                  stroke-width="0.5"
                  stroke-dasharray="2 2"
                  d="M 50 30 
                     A 15 20 0 1 1 50 70 
                     A 15 20 0 1 1 50 30 Z">
            </path>
        </svg>

        <canvas id="detection-canvas"></canvas>

    </div>

    <script type="module">
        // Importamos las clases necesarias (Solo funciona con <script type="module">)
        const { FaceDetector, FilesetResolver, RunningMode } = self.MediaPipeTasksVision;

        const videoElement = document.getElementById('camera-preview');
        const canvas = document.getElementById('detection-canvas');
        const loadingMessage = document.getElementById('loading-message');
        const context = canvas.getContext('2d');

        let faceDetector;
        let lastVideoTime = -1;
        let runningMode = RunningMode.VIDEO;

        // --- PASO 2: Inicializar y Configurar MediaPipe ---
        async function initializeFaceDetector() {
            loadingMessage.innerText = "Descargando modelos y WASM...";
            
            // Resolvemos la ruta a los archivos WASM
            const vision = await FilesetResolver.forVisionTasks(
                "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm"
            );
            
            // PASO 3: Crear el detector con opciones
            faceDetector = await FaceDetector.createFromOptions(
                vision,
                {
                    baseOptions: {
                        // ** DEBES CAMBIAR ESTO: Ruta a tu modelo descargado **
                        modelAssetPath: "./models/face_detector.tflite" 
                    },
                    runningMode: runningMode,
                    minDetectionConfidence: 0.7 
                }
            );

            loadingMessage.innerText = "¬°Detecci√≥n lista! Iniciando c√°mara...";
            console.log("MediaPipe Face Detector inicializado.");
        }

        // --- Iniciar la C√°mara ---
        async function startCamera() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                videoElement.srcObject = stream;
                // Esperamos el evento 'loadeddata' para asegurar que el video est√° listo
                videoElement.addEventListener('loadeddata', () => {
                    // Una vez cargado, ajustamos el canvas al tama√±o del video
                    canvas.width = videoElement.videoWidth;
                    canvas.height = videoElement.videoHeight;
                    videoElement.play();
                    requestAnimationFrame(detectAndDraw); // Empezamos el bucle
                });
            } catch (error) {
                console.error("Error al acceder a la c√°mara: ", error);
                loadingMessage.innerText = "‚ùå Error: Aseg√∫rate de tener c√°mara y otorgar permisos.";
            }
        }

        // --- PASO 4 y 5: Bucle de Detecci√≥n y Dibujo (detectForVideo) ---
        function detectAndDraw(now) {
            if (!faceDetector || !videoElement.currentTime || !videoElement.videoWidth) {
                // Si el detector no est√° listo o el video no tiene datos, intentamos de nuevo.
                requestAnimationFrame(detectAndDraw);
                return;
            }

            let results = null;
            // Usamos el currentTime como el timestamp, como recomienda MediaPipe
            if (videoElement.currentTime !== lastVideoTime) {
                // Ejecutar la detecci√≥n en modo VIDEO. Esto no bloquea si usa Web Worker
                results = faceDetector.detectForVideo(videoElement, performance.now());
                lastVideoTime = videoElement.currentTime;
            }

            // Limpiamos el canvas antes de dibujar
            context.clearRect(0, 0, canvas.width, canvas.height);

            // Si hay resultados, dibujamos
            if (results && results.detections.length > 0) {
                loadingMessage.style.display = 'none'; // Ocultamos el mensaje
                
                for (const detection of results.detections) {
                    const box = detection.boundingBox;
                    
                    // --- MANEJO DEL VOLTEO ---
                    // El video est√° volteado (mirror). MediaPipe da coords del video original.
                    // Para que el recuadro coincida con el video volteado:
                    const mirroredX = canvas.width - box.originX - box.width;
                    
                    // Dibujamos el recuadro de detecci√≥n (Bounding Box)
                    context.strokeStyle = "#007bff";
                    context.lineWidth = 3;
                    context.strokeRect(
                        mirroredX, // Posici√≥n X corregida
                        box.originY, 
                        box.width, 
                        box.height
                    );

                    // Opcional: Dibujamos los 6 Keypoints
                    context.fillStyle = "red";
                    for (const keypoint of detection.keypoints) {
                        const mirroredKeypointX = canvas.width - (keypoint.x * canvas.width);
                        const keypointY = keypoint.y * canvas.height;
                        context.beginPath();
                        context.arc(mirroredKeypointX, keypointY, 3, 0, 2 * Math.PI);
                        context.fill();
                    }
                }
            }

            // Continuamos el bucle en el siguiente cuadro de animaci√≥n
            requestAnimationFrame(detectAndDraw);
        }

        // --- Funci√≥n Principal ---
        async function main() {
            await initializeFaceDetector();
            await startCamera();
        }

        main();
    </script>
</body>
</html>