<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Webcam con Detección de Rostro</title>
    
    <script defer src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>

    <style>
        /* === Estilos Generales === */
        body {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            min-height: 100vh;
            margin: 0;
            background-color: #f0f0f0;
            font-family: sans-serif;
        }

        /* === Contenedor de la Cámara y el Overlay === */
        .camera-container {
            position: relative;
            width: 100%;
            max-width: 800px;
            overflow: hidden;
            border-radius: 10px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
            aspect-ratio: 4 / 3; 
        }

        /* === Elemento de Video (Cámara) === */
        #camera-preview {
            width: 100%;
            height: 100%;
            object-fit: cover;
            display: block;
            transform: scaleX(-1); /* Volteo de espejo */
        }

        /* === Overlay SVG (El hueco) === */
        #overlay-svg {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            pointer-events: none; 
        }

        /* PASO 3: Estilos para el Canvas de Detección 
          Lo posicionamos exactamente sobre el video y el SVG
        */
        #detection-canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            pointer-events: none;
            /* ¡CLAVE! Volteamos el canvas igual que el video */
            transform: scaleX(-1); 
        }

    </style>
</head>
<body>

    <h1>Detección de Rostro en Vivo</h1>
    <p style="text-align: center;">(Espera unos segundos a que carguen los modelos)</p>
    
    <div class="camera-container">
        
        <video id="camera-preview" autoplay playsinline muted></video>

        <svg id="overlay-svg" viewBox="0 0 100 100" preserveAspectRatio="none">
            <path fill-rule="evenodd" 
                  fill="rgba(255, 255, 255, 0.85)"
                  d="M 0 0 H 100 V 100 H 0 Z 
                     M 50 30 
                     A 15 20 0 1 1 50 70 
                     A 15 20 0 1 1 50 30 Z">
            </path>
            <path fill="none"
                  stroke="#007bff"
                  stroke-width="0.5"
                  stroke-dasharray="2 2"
                  d="M 50 30 
                     A 15 20 0 1 1 50 70 
                     A 15 20 0 1 1 50 30 Z">
            </path>
        </svg>

        <canvas id="detection-canvas"></canvas>

    </div>

    <script>
        // === JavaScript (¡Grandes cambios aquí!) ===

        const videoElement = document.getElementById('camera-preview');
        const canvas = document.getElementById('detection-canvas');
        
        // URL de donde cargaremos los modelos de face-api.js
        const MODEL_URL = 'https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/weights';

        // --- PASO 2: Cargar los Modelos ---
        // Esta función carga los modelos necesarios para la detección
        async function loadModels() {
            try {
                // Usaremos "Tiny Face Detector" que es el más rápido para web
                await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
                // También cargamos los "landmarks" (puntos de la cara) para dibujar mejor
                await faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL);
                console.log("Modelos de face-api.js cargados correctamente.");
            } catch (error) {
                console.error("Error al cargar los modelos de face-api.js: ", error);
            }
        }

        // --- Iniciar la Cámara (Modificado) ---
        async function startCamera() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                videoElement.srcObject = stream;
                // No llamamos a play() aquí, esperamos al evento 'play'
            } catch (error) {
                console.error("Error al acceder a la cámara: ", error);
                alert("Error al acceder a la cámara. Asegúrate de dar permisos.");
            }
        }

        // --- PASO 4 y 5: Loop de Detección y Dibujo ---
        async function startFaceDetection() {
            console.log("Iniciando detección de rostros...");

            // Sincroniza el tamaño del canvas con el tamaño real del video
            const displaySize = { width: videoElement.clientWidth, height: videoElement.clientHeight };
            faceapi.matchDimensions(canvas, displaySize);

            // Creamos el loop que se ejecuta cada 100ms
            setInterval(async () => {
                try {
                    // 1. Detectar caras
                    const detections = await faceapi.detectAllFaces(
                        videoElement, 
                        new faceapi.TinyFaceDetectorOptions()
                    ).withFaceLandmarks(); // Incluimos los puntos faciales

                    // 2. Ajustar el tamaño de las detecciones al tamaño del canvas
                    const resizedDetections = faceapi.resizeResults(detections, displaySize);

                    // 3. Limpiar el canvas antes de dibujar de nuevo
                    canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);

                    // 4. Dibujar las detecciones (el cuadro)
                    faceapi.draw.drawDetections(canvas, resizedDetections);
                    
                    // 5. Dibujar los puntos de la cara (ojos, boca, etc.)
                    faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);

                } catch (error) {
                    console.error("Error en el loop de detección: ", error);
                }
            }, 100); // Se ejecuta cada 100 milisegundos
        }

        // --- Función Principal para orquestar todo ---
        async function main() {
            // Primero cargamos los modelos
            await loadModels();
            
            // Luego iniciamos la cámara
            await startCamera();

            // Añadimos un 'listener'. Cuando el video empiece a reproducirse...
            videoElement.addEventListener('play', () => {
                // ...iniciamos el loop de detección de rostros
                startFaceDetection();
            });
        }

        // ¡Ejecutamos la función principal!
        main();

    </script>
</body>
</html>